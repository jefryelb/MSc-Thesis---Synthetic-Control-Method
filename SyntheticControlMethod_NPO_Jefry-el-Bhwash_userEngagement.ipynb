{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "cell_execution_strategy": "setup",
   "provenance": [],
   "name": "JB_SCM4.ipynb"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Synthetic Control Method\n",
    "This notebook is part of the Research Project done by Jefry el Bhwash as a final project in the MSc. Track: Applied Data Science.\n",
    "It was done in cooperation with Utrecht University and Nederlandse Publieke Omroep.\n",
    "\n",
    "It involves the steps taken to implement the Synthetic Control Method to measure the impact of a feature release on user engagement.\n",
    "\n",
    "Because this was done in cooperation with a company any market sensitve information was removed from the outputs. This makes it easy for an employee to re-run the code and see the information, while hiding it from others who are interested in the implementation.\n",
    "This is the reason why some entire code-blocks are commented.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "7SQHy0YSW8qN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Variables of interest\n",
    "start_date = '2024-03-15' # @param {type:\"date\"}\n",
    "end_date = '2024-05-22' # @param {type:\"date\"}\n",
    "date_of_feature = \"2024-04-10\" # @param {type:\"date\"}\n",
    "feature_of_interest = 'ectr' # @param {type:\"string\"}\n",
    "percentage_cutoff = 0.5 # @param {type:\"number\"}"
   ],
   "metadata": {
    "id": "DFF32t-AW70H",
    "cellView": "form"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Imports required to run this project\n",
    "\n",
    "#BigFrames\n",
    "import bigframes.pandas as bf\n",
    "#Variables of Importance\n",
    "bf.options.bigquery.location = \"EU\" #this variable is set based on the dataset you chose to query\n",
    "bf.options.bigquery.project = \"datamart-ads-students-2024\" #this variable is set based on the dataset you chose to query\n",
    "\n",
    "#Standard\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "# SCM\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.utils.validation import (check_X_y, check_array, check_is_fitted)\n",
    "from sklearn.metrics import r2_score\n",
    "import cvxpy as cp\n",
    "from toolz import partial\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "#Visualizations\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import matplotlib.ticker as ticker"
   ],
   "metadata": {
    "id": "QyNHd0SAXE6b"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Settings for matplotlib for plots in the thesis\n",
    "font = {'family' : 'sans-serif',\n",
    "        'weight' : 'regular',\n",
    "        'size'   : 11}\n",
    "\n",
    "mpl.rc('font', **font)\n",
    "plt.rcParams[\"svg.fonttype\"] = 'path'"
   ],
   "metadata": {
    "id": "mbSVd6lc7I5o"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#This is a dictionary to create a sankey diagram of the processing steps\n",
    "sankeyData = {}"
   ],
   "metadata": {
    "id": "6SSmJXGDT0UY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "metadata": {
    "id": "w12mxfoBXXqo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#SQL Query to access the database\n",
    "outcome_query_ECTR = \"\"\"\n",
    "SELECT\n",
    "  DATE(E.logged_timestamp_minute) as timestamp_date,\n",
    "  E.subscription,\n",
    "  E.platform_type,\n",
    "  S.operating_system,\n",
    "  COUNT(E.rec_panel_location) AS count_location,\n",
    "  COUNT(E.has_view) as total_view,\n",
    "  AVG(E.total_minutes_watched) as avg_total_mins_watched,\n",
    "  AVG(S.streamduration_seconds) as avg_streaming_duration_seconds,\n",
    "  MIN(E.total_minutes_watched) as min_total_minutes,\n",
    "  MAX(E.total_minutes_watched) as max_total_minutes,\n",
    "  SUM(CASE WHEN E.total_minutes_watched>=1.0 THEN 1 ELSE 0 END) as engaged_views,\n",
    "  COUNT(E.has_view)/COUNT(E.rec_panel_location) as ctr,\n",
    "  SUM(CASE WHEN E.total_minutes_watched>=1.0 THEN 1 ELSE 0 END)/ NULLIF(COUNT(E.has_view),0) as ectr --defined similarly as the dashboard table\n",
    "\n",
    "FROM `datamart-ads-students-2024.npo_intermediary_topspin_engaged_impressions.v1` AS E\n",
    "LEFT JOIN `datamart-ads-students-2024.npo_intermediary_topspin_streams_daily.v1` AS S\n",
    "ON E.session_id = S.session_id\n",
    "\n",
    "WHERE\n",
    "  DATE(E.logged_timestamp_minute) >= \"2024-01-01\" AND\n",
    "  DATE(E.logged_timestamp_minute) <= \"2024-05-30\" AND\n",
    "  DATE(S.startdate_streamstart) >= \"2024-01-01\" AND\n",
    "  DATE(S.startdate_streamstart) <= \"2024-05-30\" AND\n",
    "  E.rec_panel_location = \"home\"\n",
    "\n",
    "GROUP BY timestamp_date, E.platform_type, S.operating_system, E.subscription\n",
    "\"\"\"\n",
    "\n",
    "#SQL Query to retrieve the information above to be more efficient in terms of\n",
    "#processing speed, cost and power.\n",
    "outcome_query_preRun = \"\"\"\n",
    "SELECT *  FROM `datamart-ads-students-2024.JB_npo_intermediary_topspin_pagepaths_and_streams.januari-may`\n",
    "\"\"\"\n",
    "\n",
    "data = bf.read_gbq(outcome_query_preRun)\n",
    "# data.head()"
   ],
   "metadata": {
    "id": "Jku0BSGHXGmJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Creating a copy of the data as a regular pandas dataframe\n",
    "df = data.to_pandas().copy(deep=False)"
   ],
   "metadata": {
    "id": "de2zlLmFXJd3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Changing the date to datetime format from PyArrow format\n",
    "df['timestamp_date'] = pd.to_datetime(df['timestamp_date'], format='%Y/%m/%d')\n",
    "df = df.rename(columns={\"timestamp_date\": \"date\"})\n",
    "\n",
    "#Sorting the dataframe\n",
    "df = df.sort_values(\"date\")\n",
    "\n",
    "# print(\"Description\")\n",
    "# display(df.describe())\n",
    "# print(\"\\n\\nHead\")\n",
    "# display(df.head())"
   ],
   "metadata": {
    "id": "MYaxw-2VXn7V"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exploratory Data Analysis and Data Cleaning"
   ],
   "metadata": {
    "id": "Ow-j6e8CYNZe"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#SANKEY\n",
    "sankeyData['complete'] = df['count_location'].sum()"
   ],
   "metadata": {
    "id": "MYOM9WZUUQFH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#FILTER: Data range filtering - so this does not further implicate the exploratory data analysis\n",
    "df = df[df['date'] > start_date]\n",
    "df = df[df['date'] < end_date]"
   ],
   "metadata": {
    "id": "vp-kyJ3kYW4u"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#SANKEY\n",
    "sankeyData['datefilter'] = df['count_location'].sum()"
   ],
   "metadata": {
    "id": "jWzcq5e0UWPB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The SCM will use certain charachteristics to dicsern between different groups. These group indications will be derived from multiple attributes.\n",
    "\n",
    "**Device type:**\n",
    "> Grouping by Operating System and Platform type, for example app users of iOS. This represents the devices that were logged.\n",
    "\n",
    "**Access type:**\n",
    ">Grouping by device type and subscription. This ultimately will be used to split the group in donor pool units and treatment units."
   ],
   "metadata": {
    "id": "UtsDQihCYgIa"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#Creating a single descriptive column for the group indicators\n",
    "df['access_type'] = df['platform_type'] + '_' + df['operating_system'] + '_' + df['subscription']\n",
    "df[\"device_type\"] = df['platform_type'] + '_' + df['operating_system']"
   ],
   "metadata": {
    "id": "R5BU04AuYfhf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Device Type Distribution"
   ],
   "metadata": {
    "id": "S1W4tQuIajrG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#Plotting the amount of users that each device type holds\n",
    "# fig = px.box(df,x=\"device_type\", y=\"count_location\", color=\"platform_type\", title=\"Amount of users each day by device type\")\n",
    "# fig.show()"
   ],
   "metadata": {
    "id": "IpnouMnVY4kI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Altered plot\n",
    "\n",
    "# Group by 'date', 'device_type', and 'platform_type' to sum 'count_location'\n",
    "grouped_df = df.groupby(['date', 'device_type', 'platform_type'])['count_location'].sum().reset_index()\n",
    "\n",
    "# Calculate the total count per day\n",
    "total_per_day = grouped_df.groupby('date')['count_location'].sum().reset_index()\n",
    "total_per_day.rename(columns={'count_location': 'total_count'}, inplace=True)\n",
    "\n",
    "# Merge the total count per day back to the grouped_df\n",
    "merged_df = pd.merge(grouped_df, total_per_day, on='date')\n",
    "\n",
    "# Calculate the percentage of each device type and platform type per day\n",
    "merged_df['percentage'] = (merged_df['count_location'] / merged_df['total_count']) * 100\n",
    "\n",
    "# Create the box plot\n",
    "fig = px.box(\n",
    "    merged_df,\n",
    "    x=\"device_type\",\n",
    "    y=\"percentage\",\n",
    "    color=\"platform_type\",\n",
    "    title=\"Percentage of users each day by device type and platform type (Aggregated)\",\n",
    "    labels={\n",
    "        \"device_type\": \"Device Type\",\n",
    "        \"percentage\": \"Percentage of Users\",\n",
    "        \"platform_type\": \"Platform Type\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Update y-axis to show percentages\n",
    "fig.update_layout(yaxis_ticksuffix=\"%\")\n",
    "\n",
    "# Update hover template to show percentages with 2 decimal places\n",
    "fig.update_traces(\n",
    "    hovertemplate=\"<br>\".join([\n",
    "        \"Device Type: %{x}\",\n",
    "        \"Platform Type: %{color}\",\n",
    "        \"Percentage: %{y:.2f}%\",\n",
    "        \"<extra></extra>\"\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ],
   "metadata": {
    "id": "fphT9R-1NmHS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# fig = px.box(df, x=\"device_type\", y=\"count_location\", color=\"platform_type\",\n",
    "#              facet_col=\"platform_type\",\n",
    "#              facet_col_wrap=2,\n",
    "#              facet_col_spacing=0.08#,  # Facet by platform_type\n",
    "#              ) # title=\"Amount of users each day by device type (Facet by Platform)\"\n",
    "# fig.update_yaxes(matches=None, showticklabels=True, title=\"Users per day\")\n",
    "# fig.update_xaxes(matches=None,tickangle=45)\n",
    "# fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n",
    "# # hide subplot y-axis titles and x-axis titles\n",
    "# for axis in fig.layout:\n",
    "#     if type(fig.layout[axis]) == go.layout.XAxis:\n",
    "#         fig.layout[axis].title.text = ''\n",
    "# fig.show()"
   ],
   "metadata": {
    "id": "2yHp1uyWHe5g"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Average Device types per day"
   ],
   "metadata": {
    "id": "1L0Oxq_4apDP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Group by 'date' and 'device_type' to sum 'count_location'\n",
    "grouped_df = df.groupby(['date', 'device_type'])['count_location'].sum().reset_index()\n",
    "\n",
    "# Calculate the total count per day\n",
    "total_per_day = grouped_df.groupby('date')['count_location'].sum().reset_index()\n",
    "total_per_day.rename(columns={'count_location': 'total_count'}, inplace=True)\n",
    "\n",
    "# Merge the total count per day back to the grouped_df\n",
    "merged_df = pd.merge(grouped_df, total_per_day, on='date')\n",
    "\n",
    "# Calculate the percentage of each device type per day\n",
    "merged_df['percentage'] = (merged_df['count_location'] / merged_df['total_count']) * 100\n",
    "\n",
    "# Group by 'device_type' and calculate the average percentage\n",
    "average_percentage_df = merged_df.groupby('device_type')['percentage'].mean().reset_index()\n",
    "\n",
    "# Sort the DataFrame by 'percentage'\n",
    "average_percentage_df = average_percentage_df.sort_values(by='percentage', ascending=False)\n",
    "\n",
    "# Plot using Plotly\n",
    "fig = px.bar(average_percentage_df, x='device_type', y='percentage',\n",
    "             title='Average Percentage of Device Types Used',\n",
    "             labels={'percentage': 'Average Percentage', 'device_type': 'Device Type'},\n",
    "             category_orders={'device_type': average_percentage_df['device_type'].tolist()})\n",
    "\n",
    "fig.show()"
   ],
   "metadata": {
    "id": "kbVxYd3ka_Ci"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#THESIS PLOT\n",
    "# Plot the bars.\n",
    "fig, ax = plt.subplots()\n",
    "x_ticks = list(range(len(average_percentage_df['percentage'])))\n",
    "ax.bar(average_percentage_df['device_type'], average_percentage_df['percentage'], color='#7a7a7a', width=0.6, zorder=1)\n",
    "\n",
    "# Remove axis lines.\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "\n",
    "# Add labels to x axis.\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels(average_percentage_df['device_type'])\n",
    "ax.tick_params(axis='x', labelrotation=90)\n",
    "\n",
    "# Add labels to y axis.\n",
    "y_ticks = [1,5, 10, 15,20,25,30]\n",
    "ax.set_yticks(y_ticks)\n",
    "ax.set_yticklabels(y_ticks)\n",
    "ax.yaxis.set_major_formatter(ticker.PercentFormatter(decimals=0))\n",
    "\n",
    "# Remove tick marks.\n",
    "ax.tick_params(\n",
    "    bottom=False,\n",
    "    left=False\n",
    ")\n",
    "\n",
    "ax.set_axisbelow(False)\n",
    "# Add bar lines as a horizontal grid.\n",
    "ax.yaxis.grid(color='white', zorder=2)\n",
    "\n",
    "plt.savefig(\"deviceTypeDist.svg\")"
   ],
   "metadata": {
    "id": "Dv_hFlLU2zy1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because some groups are representing less than a percent of the userbase it might be wise to remove them from the analysis. This is done because the behaviour of the little amount of users from a single platform will be given the same weight inititally. Therefore the choice is made to remove them from the analysis."
   ],
   "metadata": {
    "id": "DD3VlKuDa3CG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#FILTER: Keep only device types where more than `percentage_cutoff` of users are residing\n",
    "deleted_device_types = list(average_percentage_df[average_percentage_df['percentage'] < percentage_cutoff][\"device_type\"])\n",
    "print(f\"Removed Devices: {deleted_device_types}\")\n",
    "df = df[~df['device_type'].isin(deleted_device_types)]"
   ],
   "metadata": {
    "id": "Bl8dAxk5bXE8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#SANKEY\n",
    "sankeyData['deviceTypeFilter'] = df['count_location'].sum()"
   ],
   "metadata": {
    "id": "5-VW_l1VUh4z"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Missing Data"
   ],
   "metadata": {
    "id": "elKlo3sXhR45"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "#Normal Matrix\n",
    "msno.matrix(df.set_index(\"date\"), sparkline=False, ax=axes[0], fontsize=12)\n",
    "axes[0].set_title('Matrix of missing values')\n",
    "#Also 0 as missing\n",
    "msno.matrix(df.replace(0, np.nan).set_index(\"date\"), sparkline=False, ax=axes[1], fontsize=12)\n",
    "axes[1].set_title('Matrix of missing- and 0-values')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "k5ZiDumGpeAQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# df"
   ],
   "metadata": {
    "id": "ACLnNbTAraYn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Enaged view and total view are sometimes 0, causing the ectr which is calculated on those to be NA, since division by 0 is impossible."
   ],
   "metadata": {
    "id": "hUX2l3c5oK5B"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Missing data imputation\n",
    "replacing each missing value with the mean of the column"
   ],
   "metadata": {
    "id": "IXFQFLkSswqm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#Impute based on the mean\n",
    "na_cols = df.columns[(df == 0).any() | (df == 1).any()]\n",
    "df[na_cols] = df[na_cols].replace({0: np.nan, 1: np.nan})\n",
    "\n",
    "# Iterate through columns\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'Float64':  # Replace NaN with mean for float columns\n",
    "        df[col].fillna(df[col].mean(), inplace=True)\n",
    "    elif df[col].dtype == 'Int64':  # Replace NaN with mode for int columns\n",
    "        df[col].fillna(df[col].mean().astype(int), inplace=True)"
   ],
   "metadata": {
    "id": "HAtamTOzswZF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# df.describe()"
   ],
   "metadata": {
    "id": "rEpduPe1z-XV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# #SANKEY\n",
    "# sankeyData['imputation'] = df['count_location'].sum()"
   ],
   "metadata": {
    "id": "iUFfg3UpUtKU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extracting Information\n",
    "We need to extract information from the data to find the treatment and control units. We also might want to use information like people accessing the platform through a website or through an application, since these would probably result in different behaviours."
   ],
   "metadata": {
    "id": "2gj1jabP0J85"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def checkAccountHolder(entry):\n",
    "  '''\n",
    "  checkAccountHolder(entry) is a function that determines\n",
    "  if a stream was seen with an account or without one\n",
    "\n",
    "  `entry` is the row of the dataframe\n",
    "\n",
    "  Anonymous should be 0\n",
    "  accounts like free and premium should be 1\n",
    "  if the account type is not one of those -1 is entered\n",
    "  '''\n",
    "  if entry[\"subscription\"] == \"anonymous\":\n",
    "    account = 0\n",
    "  elif entry[\"subscription\"] == \"free\":\n",
    "    account = 1\n",
    "  elif entry[\"subscription\"] == \"premium\":\n",
    "    account = 1\n",
    "  elif entry[\"subscription\"] == \"plus\":\n",
    "    account = 1\n",
    "  else:\n",
    "    account = -1\n",
    "  return account\n",
    "\n",
    "\n",
    "#Applying the above function checkAccountHolder\n",
    "#   to get part of the criteria for the intervention group\n",
    "df['account_holder'] = df.apply(checkAccountHolder, axis=1)"
   ],
   "metadata": {
    "id": "cJ8mMTIB0NWX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Weekend Variable\n",
    "# 1 if weekend (sat, sun) [5,6]\n",
    "# 0 if weekday (mom, tue, wed, thu, fri) [0,1,2,3,4]\n",
    "\n",
    "def weekend_logic(row):\n",
    "  '''\n",
    "  A function that returns 1 if the day of the week is\n",
    "  fri, sat, sun\n",
    "  '''\n",
    "  if row['date'].weekday() > 4:\n",
    "    weekend = 1\n",
    "  elif row['date'].weekday() < 5:\n",
    "    weekend = 0\n",
    "  else:\n",
    "    weekend = -1\n",
    "  return weekend\n",
    "\n",
    "df[\"weekend\"] = df.apply(weekend_logic, axis=1)"
   ],
   "metadata": {
    "id": "y3SOM9jV0QNt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Website Variable\n",
    "# Only 1 if the user is using the website through a desktop or laptop, not through a phone.\n",
    "website_OS = [\"Mac OS X\", \"Windows\", \"Linux\", \"Ubuntu\", \"Chrome OS\", \"WebOS\"]\n",
    "def website_logic(row):\n",
    "  '''\n",
    "  A function that returns 1 if the platform was website\n",
    "  '''\n",
    "  if row['platform_type'] == \"site\" and row[\"operating_system\"] in website_OS:\n",
    "    site = 1\n",
    "  else:\n",
    "    site = 0\n",
    "  return site\n",
    "\n",
    "df[\"site\"] = df.apply(website_logic, axis=1)"
   ],
   "metadata": {
    "id": "SlbqPo_F0RPh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# df.head()"
   ],
   "metadata": {
    "id": "GFrfMcwt0TbS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataframe preparation for the Synthetic Control Model\n"
   ],
   "metadata": {
    "id": "xTioP2y-0ZDj"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Intervention Group:\n",
    "\n",
    ">Feature: Add or Remove followed shows\n",
    "\n",
    "Platform to the feature:\n",
    "* `Web`: 2024-04-10\n",
    "* `Android TV`: 2024-05-22\n",
    "* `Android Mobile` 2024-05-22\n",
    "* `iOS mobile` & `Apple TV`: NA\n",
    "\n",
    "So if data is taken till 2024-05-22, which is enough time to measure the effect. We can include Android and iOS users as well.\n",
    "\n",
    "**intervention group:** web users with an account\n",
    "\n",
    "**donor pool:** app users with an account and all anonymous users"
   ],
   "metadata": {
    "id": "QPzA1B8v11oQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Create a column that is called Intervention based on the following criteria:\n",
    "#   platform = site\n",
    "#   account holder = 1\n",
    "#   otherwise 0 - this will be our control group\n",
    "\n",
    "def intervention_attribute(row):\n",
    "  '''\n",
    "  intervention_attribute(row) is a function that checks\n",
    "  if a user is from the group that has gotten the intervention\n",
    "  '''\n",
    "  if row[\"platform_type\"] == \"site\" and row[\"account_holder\"] == 1:\n",
    "    intervention = 1 #people with an account using the website platform\n",
    "  elif row[\"platform_type\"] == \"app\" and row[\"account_holder\"] == 1:\n",
    "    intervention = 0 #people with an account using the applications\n",
    "  elif row[\"subscription\"] == \"anonymous\":\n",
    "    intervention = 0 #anonymous users of all platforms\n",
    "  else:\n",
    "    intervention = -1 #people with an account on different platforms\n",
    "  return intervention\n",
    "\n",
    "\n",
    "#Applying the intervention function to be able to split the groups\n",
    "df['intervention'] = df.apply(intervention_attribute, axis=1)"
   ],
   "metadata": {
    "id": "kmEh3byd0PNb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Creating a new dataframe based on the intervention group\n",
    "df_scm = df.loc[df['intervention']!=-1]"
   ],
   "metadata": {
    "id": "3CVf9lQk0cnq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# df_scm.head()"
   ],
   "metadata": {
    "id": "eQEFm2-2iq5T"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"Rate of site users in this dataframe: {df_scm['site'].sum()/df_scm['site'].shape[0]}\")"
   ],
   "metadata": {
    "id": "icT0n-8_0fpk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Synthetic Control Method\n",
    "The method is very well explained in different resources.\n",
    ">In short we are going to emulate an **Random Control Trial** by synthetically recreating a control group using the groups we have assigned to the donor pool.\n",
    "\n",
    "The book [Facure, 2023] has been especially helpfull in translating the (sometimes daunting) mathematics of the artical [Abadie, 2021] to the Python scripts we find below. Almost all of the scripts mentioned in the book don't work immediately so they have been altered to work for this application, while keeping the original work of Abadie in mind."
   ],
   "metadata": {
    "id": "AqdecG4x0mL-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Splitting the Data"
   ],
   "metadata": {
    "id": "6CvblpK1HFET"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# This function splits the data in 4 sets:\n",
    "#  1) Pre-intervention Control\n",
    "#  2) Pre-intervention treatment\n",
    "#  3) post intervention Control\n",
    "#  4) Post intervention Treatment\n",
    "\n",
    "def reshape_sc_data_original(df: pd.DataFrame, #Dataframe\n",
    "                    geo_col: str, #Group definition\n",
    "                    time_col: str, #Time column\n",
    "                    y_col: str, #Variable of interest\n",
    "                    tr_geos: str, #Treatment definition\n",
    "                    tr_start: str): #Treament date\n",
    "\n",
    "  #Pivotting the dataframe\n",
    "  df_pivot = df.pivot_table(index=time_col, columns=geo_col,values= y_col)\n",
    "\n",
    "  #Only selecting columns that are present in the dataframe\n",
    "  tr_geos = list(set(df_pivot.columns) & set(tr_geos))\n",
    "\n",
    "  #Splitting the pivoted group in Control and Treatment units\n",
    "  y_co = df_pivot.drop(columns=tr_geos).dropna(axis=1)\n",
    "  y_tr = df_pivot[tr_geos].dropna(axis=1)\n",
    "\n",
    "  #Splitting the group by pre-intervention times and post intervention times\n",
    "  y_pre_co = y_co[df_pivot.index < tr_start]\n",
    "  y_pre_tr = y_tr[df_pivot.index < tr_start]\n",
    "\n",
    "  y_post_co = y_co[df_pivot.index >= tr_start]\n",
    "  y_post_tr = y_tr[df_pivot.index >= tr_start]\n",
    "\n",
    "  return y_pre_co, y_pre_tr, y_post_co, y_post_tr"
   ],
   "metadata": {
    "id": "BEWgCJL70l4b"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Creating a list that contains the access_types of all the treatment unit groups\n",
    "treatment_units = list(df_scm.query(\"intervention==1\")[\"access_type\"].unique())\n",
    "\n",
    "#Split groups\n",
    "y_pre_co, y_pre_tr, y_post_co, y_post_tr = reshape_sc_data_original(df = df_scm,\n",
    "                                                            geo_col = \"access_type\",\n",
    "                                                            time_col = \"date\",\n",
    "                                                            y_col = feature_of_interest,\n",
    "                                                            tr_geos = treatment_units,\n",
    "                                                            tr_start = date_of_feature)\n",
    "# display(y_pre_tr.head())"
   ],
   "metadata": {
    "id": "u27Zv_uP0qSK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualizing the split data\n",
    "The treatment group exists of multiple device and access types.\n",
    "Choosing between the mean of all of these or picking a single access_type will be done based on the visualization below."
   ],
   "metadata": {
    "id": "ab0nzPQF8JOm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# #Figure of control vs treatment\n",
    "\n",
    "# # Data preparation\n",
    "# df_pivot = df_scm.pivot_table(index='date', columns='access_type',values= feature_of_interest)\n",
    "\n",
    "# treatment_unit_droplist = list(set(df_pivot.columns) & set(treatment_units)) #making sure these columns are in the dataframe\n",
    "\n",
    "# ## Splitting in control and treatment units\n",
    "# y_co = df_pivot.drop(columns=treatment_unit_droplist) #dropping the treatment units\n",
    "# y_tr = df_pivot[treatment_unit_droplist] #keeping the treatment units\n",
    "\n",
    "# # Figure building\n",
    "# fig = go.Figure()\n",
    "\n",
    "# # The control group\n",
    "# for column in y_co.columns:\n",
    "#     fig.add_trace(go.Scatter(\n",
    "#         x=y_co.index,\n",
    "#         y=y_co[column],\n",
    "#         mode='lines',\n",
    "#         name=f'{column}',\n",
    "#         line=dict(color=\"orange\", dash='dash'),\n",
    "#         opacity=1\n",
    "#     ))\n",
    "# # The treatment group\n",
    "# for column in y_tr.columns:\n",
    "#     fig.add_trace(go.Scatter(\n",
    "#         x=y_tr.index,\n",
    "#         y=y_tr[column],\n",
    "#         mode='lines',\n",
    "#         name=f'{column}',\n",
    "#         line=dict(color=\"blue\", dash='dash'),\n",
    "#         opacity = 1\n",
    "#     ))\n",
    "\n",
    "# # The mean of the treatment group\n",
    "# fig.add_trace(go.Scatter(\n",
    "#     x=y_tr.index,\n",
    "#     y=y_tr.mean(axis=1),\n",
    "#     mode='lines+markers',\n",
    "#     name='Mean of treatment Group',\n",
    "#     line=dict(color='black', width=2)\n",
    "# ))\n",
    "\n",
    "# # Change range and title of the plot\n",
    "# fig.update_layout(\n",
    "#     yaxis=dict(range=[0, 1]),\n",
    "#     title=\"ECTR per Access type\"\n",
    "# )\n",
    "\n",
    "# fig.show()"
   ],
   "metadata": {
    "id": "dAh841jM1Guv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# #THESIS PLOT\n",
    "\n",
    "# # Data preparation\n",
    "# df_pivot = df_scm.pivot_table(index='date', columns='access_type',values= feature_of_interest)\n",
    "\n",
    "# treatment_unit_droplist = list(set(df_pivot.columns) & set(treatment_units)) #making sure these columns are in the dataframe\n",
    "\n",
    "# ## Splitting in control and treatment units\n",
    "# y_co = df_pivot.drop(columns=treatment_unit_droplist) #dropping the treatment units\n",
    "# y_tr = df_pivot[treatment_unit_droplist] #keeping the treatment units\n",
    "\n",
    "\n",
    "# # Function to plot a series, skipping NaN values\n",
    "# def plot_series(ax, x, y, color, label, alpha=1.0):\n",
    "#     mask = ~np.isnan(y)\n",
    "#     ax.plot(x[mask], y[mask], color=color, label=label, alpha=alpha)\n",
    "\n",
    "# # Create subplot\n",
    "# fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# # Plot y_co dataframe columns in orange\n",
    "# # for column in y_co.columns:\n",
    "# #     plot_series(ax, y_co.index, y_co[column], 'tab:orange', f'y_co_{column}', alpha=0.3)\n",
    "\n",
    "# # # Plot y_tr dataframe columns in blue\n",
    "# for column in y_tr.columns:\n",
    "#     plot_series(ax, y_tr.index, y_tr[column], 'tab:blue', f'y_tr_{column}', alpha=0.3)\n",
    "\n",
    "# # Calculate and plot the mean of y_tr in black\n",
    "# y_tr_mean = y_tr.mean(axis=1)\n",
    "# plot_series(ax, y_tr.index, y_tr_mean, 'black', 'y_tr_mean')\n",
    "\n",
    "# #DateLine\n",
    "# plt.axvline(datetime.datetime(2024,4,10), color = 'black', linestyle = 'dashed')\n",
    "# # Customize the plot\n",
    "# ax.set_title('The Treatment Units')\n",
    "# ax.set_ylabel('ECTR')\n",
    "# ax.set_xlim([datetime.datetime(2024,3,16), datetime.datetime(2024,5,21)])\n",
    "# ax.set_ylim(0,1)\n",
    "# plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# # Show the plot\n",
    "# plt.tight_layout()\n",
    "# # plt.savefig(\"treatmentMeanAndDonor.svg\")\n",
    "# plt.savefig(\"treatmentMeanAndTreatments.svg\")\n",
    "# plt.show()"
   ],
   "metadata": {
    "id": "Q9J5LDxXKbLH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exclusion of some groups\n",
    "As mentioned in [Abadie, 2021] in the Contextual Requirements chapter it is important to have a comparison group available;\n",
    "\n",
    "> “Moreover, it is important to restrict the donor pool to units with characteristics that are similar to the affected unit. The reason is that, while the restrictions placed on the weights, W, do not allow extrapolation, interpolation biases may still be important if the synthetic control matches the characteristics of the affected unit by averaging away large discrepancies between the characteristics of the affected unit and the characteristics of the units in the synthetic control.”\n",
    ">\n",
    ">&mdash; <cite>(Abadie, 2021, p. 409)</cite>"
   ],
   "metadata": {
    "id": "ZvztlQ_9fjAt"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#These groups are removed because they behave very differently from the other groups\n",
    "manual_exclusion = ['app_Android_plus',\n",
    "                    'app_Chromecast_plus',\n",
    "                    'tvapp_Android_anonymous',\n",
    "                    'site_Mac OS X_free',\n",
    "                    'site_Chrome OS_free',\n",
    "                    'site_Chrome OS_premium',\n",
    "                    'site_iOS_premium',\n",
    "                    'site_Android_premium',\n",
    "                    'site_iOS_free',\n",
    "                    'site_Android_free']"
   ],
   "metadata": {
    "id": "Ll3lBFjDUvAi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#SANKEY\n",
    "sankeyData['manualExclusion'] = df[~df['access_type'].isin(manual_exclusion)]['count_location'].sum()"
   ],
   "metadata": {
    "id": "friGJDcPU36d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Split groups\n",
    "y_pre_co, y_pre_tr, y_post_co, y_post_tr = reshape_sc_data_original(df = df_scm[~df_scm['access_type'].isin(manual_exclusion)],\n",
    "                                                            geo_col = \"access_type\",\n",
    "                                                            time_col = \"date\",\n",
    "                                                            y_col = feature_of_interest,\n",
    "                                                            tr_geos = treatment_units,\n",
    "                                                            tr_start = date_of_feature)\n",
    "# display(y_pre_tr.head())"
   ],
   "metadata": {
    "id": "Q9MQOMMbV1yQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model\n",
    "This section will describe the creation of the synthetic control using the donor pool units that were assigned above.\n",
    "\n"
   ],
   "metadata": {
    "id": "CXHOlTyC4Pjd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Setting the Treatment to Mean or Single Access Type\n",
    "y_pre_tr_variable = y_pre_tr.mean(axis=1) # @param [\"y_pre_tr.mean(axis=1)\", \"y_pre_tr[\\\"site_Windows_premium\\\"]\"] {type:\"raw\"}\n",
    "y_post_tr_variable = y_post_tr.mean(axis=1) # @param [\"y_post_tr.mean(axis=1)\", \"y_post_tr[\\\"site_Windows_premium\\\"]\"] {type:\"raw\"}"
   ],
   "metadata": {
    "id": "C66spedk5F0f",
    "cellView": "form"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#The Synthetic Control Method class\n",
    "class SyntheticControl(BaseEstimator, RegressorMixin):\n",
    "  def __init__(self,):\n",
    "    pass\n",
    "\n",
    "  #This fit function determines the weights for the units.\n",
    "  def fit(self, y_pre_co, y_pre_tr):\n",
    "\n",
    "    #This is not required, but checks the dimensions of X and Y.\n",
    "    y_pre_co, y_pre_tr = check_X_y(y_pre_co, y_pre_tr)\n",
    "\n",
    "    #Initializing the weight vector.\n",
    "    w = cp.Variable(y_pre_co.shape[1])\n",
    "\n",
    "    #The objective is to minimize the error, similar to OLS\n",
    "    #  However there are some constraints put in place, making it different.\n",
    "    objective = cp.Minimize(cp.sum_squares(y_pre_co@w - y_pre_tr))\n",
    "\n",
    "    #These constraint make sure there will be no extrapolation.\n",
    "    constraints = [cp.sum(w) == 1, w >= 0]\n",
    "\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "\n",
    "    self.loss_ = problem.solve(verbose=False)\n",
    "    self.w_ = w.value\n",
    "\n",
    "    self.is_fitted_ = True\n",
    "    return self\n",
    "\n",
    "\n",
    "    def predict(self, y_co):\n",
    "\n",
    "      check_is_fitted(self)\n",
    "      y_co = check_array(y_co)\n",
    "\n",
    "      return y_co @ self.w_"
   ],
   "metadata": {
    "id": "bfjeL_eK4c-r"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Apply the model\n",
    "model = SyntheticControl()\n",
    "model.fit(y_pre_co, y_pre_tr_variable)\n",
    "\n",
    "#the weights are stored in w_\n",
    "model.w_.round(3)"
   ],
   "metadata": {
    "id": "LkRtDONS4exB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fit of the Model"
   ],
   "metadata": {
    "id": "-rG_Bq2xiXqm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "y0_tr_hat     = y_post_co.dropna(axis=1).dot(model.w_) #Synthetic Control (=donor pool post treatment X its weights)\n",
    "y0_tr_hat_fit = y_pre_co.dropna(axis=1).dot(model.w_) #Fit of the synthetic control (=donor pool pre treatment X its weights)"
   ],
   "metadata": {
    "id": "fflrWB1X4ikj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"R2 of the fit: {round(r2_score(y_pre_tr_variable, y0_tr_hat_fit),3)}\")\n",
    "\n",
    "#Combined Plot\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "\n",
    "#PreTreatment\n",
    "plt.plot(y_pre_tr_variable.index, y_pre_tr_variable.values, label=\"Treatment Group\", color='tab:blue')\n",
    "plt.plot(y0_tr_hat_fit.index, y0_tr_hat_fit.values, linestyle='dashed', label=\"Fit of the method\", color='tab:red', alpha=0.5)\n",
    "\n",
    "#PostTreatment\n",
    "plt.plot(y_post_tr_variable.index, y_post_tr_variable.values, color='tab:blue')\n",
    "plt.plot(y0_tr_hat.index, y0_tr_hat.values, label=\"Synthetic Control\", linestyle='dashed', color='tab:orange', alpha=1)\n",
    "\n",
    "#DateLine\n",
    "plt.axvline(datetime.datetime(2024,4,10), color = 'black', linestyle = 'dashed')\n",
    "\n",
    "# plt.xlabel('Date')\n",
    "plt.ylabel(feature_of_interest)\n",
    "\n",
    "\n",
    "plt.title('ECTR\\nThe Synthetic Control and Intervention Group')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(axis='y')\n",
    "plt.ylim(0,0.6)\n",
    "plt.xlim([datetime.datetime(2024,3,16), datetime.datetime(2024,5,21)])\n",
    "plt.tight_layout()\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "\n",
    "plt.savefig(\"ECTR_normalFit.svg\")\n",
    "# plt.show()"
   ],
   "metadata": {
    "id": "0golqMcL4leS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Adding a Covariate\n",
    "The model could use more information than solely the ECTR of each unit. We are going to see if adding the variable \"site\" to the model. Site is chosen because it offers the model information about the behaviour of the unit. Since as established earlier, we expect that the desktop/laptop usage will be different than other types of usage."
   ],
   "metadata": {
    "id": "VLl23cqB7to1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#Adding a variable in long format\n",
    "reshaper = partial(reshape_sc_data_original,\n",
    "                    df=df_scm,\n",
    "                    geo_col=\"access_type\",\n",
    "                    time_col=\"date\",\n",
    "                    tr_geos=treatment_units,\n",
    "                    tr_start=\"2024-04-10\")\n",
    "\n",
    "y_pre_co, y_pre_tr, y_post_co, y_post_tr = reshaper(\n",
    "    y_col=feature_of_interest\n",
    ")\n",
    "\n",
    "x_pre_co, _, x_post_co, _ = reshaper(y_col=\"site\")"
   ],
   "metadata": {
    "id": "Ocyz4mmf6W7e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# A function that finds the optimal fit of the model,\n",
    "# based on the weights of the two covariates\n",
    "def find_w_given_vs(vs, x_co_list, y_tr_pre):\n",
    "\n",
    "    #X are the covariates, v is the weight assigned to the covariates\n",
    "    X_times_v = sum([x*v for x, v in zip(x_co_list, vs)])\n",
    "\n",
    "    model = SyntheticControl()\n",
    "    model.fit(X_times_v, y_tr_pre)\n",
    "\n",
    "    return {\"loss\": model.loss_, \"w\": model.w_}"
   ],
   "metadata": {
    "id": "L_DQ9zFo6YT4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def v_loss(vs):\n",
    "  return find_w_given_vs(vs,\n",
    "                        [y_pre_co, x_pre_co],\n",
    "                         y_pre_tr_variable).get(\"loss\")\n",
    "\n",
    "v_solution = minimize(v_loss, [0, 0], method='L-BFGS-B')\n",
    "v_solution.x"
   ],
   "metadata": {
    "id": "XfCykZ3t6iTj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#The original Weights by implying that the importance of the second feature is 0\n",
    "find_w_given_vs([1, 0],\n",
    "                [y_pre_co, x_pre_co],\n",
    "                y_pre_tr_variable).get(\"w\").round(3)"
   ],
   "metadata": {
    "id": "uagtcY1fjW8W"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#New weights for the donor pool units, when adding the new covariate\n",
    "w_cov = find_w_given_vs(v_solution.x,\n",
    "                        [y_pre_co, x_pre_co],\n",
    "                        y_pre_tr_variable).get(\"w\").round(3)\n",
    "w_cov"
   ],
   "metadata": {
    "id": "StslK8__6oQ0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "y0_hat_cov = sum([x*v for x, v in zip([y_post_co, x_post_co], v_solution.x)]).dot(w_cov)\n",
    "y0_hat_fit_cov =sum([x*v for x, v in zip([y_pre_co, x_pre_co], v_solution.x)]).dot(w_cov)"
   ],
   "metadata": {
    "id": "JZS37Oc96s2x"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"R2 of the fit: {round(r2_score(y_pre_tr_variable, y0_hat_fit_cov),3)}\")\n",
    "#Combined Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "#PreTreatment\n",
    "plt.plot(y_pre_tr_variable.index, y_pre_tr_variable.values, label=\"Treatment Group\", color='tab:blue', marker=\".\")\n",
    "plt.plot(y0_hat_fit_cov.index, y0_hat_fit_cov.values, label=\"Fit of Synthetic control on Treatment\", linestyle='dashed', color='tab:red', alpha=0.5)\n",
    "\n",
    "#PostTreatment\n",
    "plt.plot(y_post_tr_variable.index, y_post_tr_variable.values, marker=\".\")\n",
    "plt.plot(y0_hat_cov.index, y0_hat_cov.values, label=\"Synthetic Control\", linestyle='dashed', color='tab:orange', alpha=0.7)\n",
    "\n",
    "#DateLine\n",
    "plt.axvline(datetime.datetime(2024,4,10), color = 'black', linestyle = 'dashed')\n",
    "\n",
    "# plt.xlabel('Date')\n",
    "plt.ylabel(feature_of_interest)\n",
    "\n",
    "plt.title('ECTR \\nThe Synthetic Control and Intervention Group with covariate: site')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.ylim(0,0.6)\n",
    "plt.xlim([datetime.datetime(2024,3,16), datetime.datetime(2024,5,21)])\n",
    "plt.grid(axis='y')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"ECTR_covariateFit.svg\")\n",
    "# plt.show()"
   ],
   "metadata": {
    "id": "HfEdXd_m6uvN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Effect of the Feature on the treatment Variable"
   ],
   "metadata": {
    "id": "zAsJItkT9I-U"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "plt.plot(y_pre_tr_variable-y0_hat_fit_cov, color='black')\n",
    "plt.plot(y_post_tr_variable-y0_hat_cov, color='black', linestyle='dashed')\n",
    "\n",
    "#Indicative Lines\n",
    "plt.axvline(datetime.datetime(2024,4,10), color = 'black', linewidth=0.5)\n",
    "plt.axhline(0, color = 'black', linewidth=0.5)\n",
    "\n",
    "plt.xlim([datetime.datetime(2024,3,16), datetime.datetime(2024,5,21)])\n",
    "plt.ylim([-0.1,0.1])\n",
    "plt.title(\"Difference between synthetic control and treatment unit\\n(with site)\")\n",
    "plt.tight_layout()\n",
    "plt.tick_params(axis='x', labelrotation=45)\n",
    "plt.savefig(\"Effect_covariateFit.svg\")\n",
    "# plt.show()"
   ],
   "metadata": {
    "id": "2RTnaobE7XOo"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preventing Overfitting by using K-fold Cross Validation\n",
    "If the donor pool units are getting close to overfitting, we could run into bias issues. Therefore it is safer to prevent the overfitting by applying K-fold cross validation on the training units.\n",
    "\n",
    "\n",
    "- ATT: Average Treatment effect of the Treated"
   ],
   "metadata": {
    "id": "z5Dpj6aU9ZSP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#Disclaimer: This function does not use the added covariate\n",
    "def debiased_sc_atts(y_pre_co, y_pre_tr, y_post_co, y_post_tr, K=3):\n",
    "    #How many days in a single block\n",
    "    block_size = int(min(np.floor(len(y_pre_tr)/K), len(y_post_tr)))\n",
    "    #The indexes of each block\n",
    "    blocks = np.split(y_pre_tr.index[-K*block_size:], K)\n",
    "\n",
    "    #The resulting lists\n",
    "    debiased_effects = []\n",
    "    block_weights = []\n",
    "\n",
    "    def fold_effect(hold_out):\n",
    "        #The same model\n",
    "        model = SyntheticControl()\n",
    "        #Fit the model on the data without one of the blocks\n",
    "        model.fit(y_pre_co.drop(hold_out), y_pre_tr.drop(hold_out))\n",
    "\n",
    "        #An estimate for the bias\n",
    "        bias_hat = np.mean(y_pre_tr.loc[hold_out] - y_pre_co.loc[hold_out].dot(model.w_))\n",
    "\n",
    "        #The weights applied to the data\n",
    "        y0_hat = y_post_co.dot(model.w_)\n",
    "\n",
    "        #Appending the weights so we can reference them later\n",
    "        block_weights.append(model.w_.round(3))\n",
    "\n",
    "        return (y_post_tr - y0_hat) - bias_hat\n",
    "\n",
    "    for block in blocks:\n",
    "        debiased_effects.append(fold_effect(block))\n",
    "\n",
    "    results = {\n",
    "        'debiased_effects': pd.DataFrame(debiased_effects).T,\n",
    "        'block_weights': block_weights\n",
    "    }\n",
    "\n",
    "    return results"
   ],
   "metadata": {
    "id": "_JkUcYslpYyn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "debiasingResults = debiased_sc_atts(y_pre_co,\n",
    "                                    y_pre_tr_variable,\n",
    "                                    y_post_co,\n",
    "                                    y_post_tr_variable,\n",
    "                                    K=6)\n",
    "deb_atts = debiasingResults[\"debiased_effects\"]\n",
    "block_weights = debiasingResults[\"block_weights\"]"
   ],
   "metadata": {
    "id": "i3M4eZVtpcY-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Plots\n",
    "plt.plot(deb_atts.mean(axis=1).index, deb_atts.mean(axis=1).values, label=\"Debiased (Cross Validated) Effect\")\n",
    "plt.plot(y_post_tr_variable.index, y_post_tr_variable-y0_hat_cov, color='black', linestyle='dashed', alpha=0.5, label=\"Original Effect\")\n",
    "\n",
    "#Indicative Lines\n",
    "plt.axvline(datetime.datetime(2024,4,10), color = 'black', linewidth=0.5)\n",
    "plt.axhline(0, color = 'black', linewidth=0.5)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.title(\"Treatment effect: Cross validation versus overfitting\")\n",
    "plt.xlim([datetime.datetime(2024,4,10), datetime.datetime(2024,5,21)])\n",
    "plt.ylim([-0.1,0.1])\n",
    "plt.tick_params(axis='x', labelrotation=45)\n",
    "\n",
    "# plt.show()"
   ],
   "metadata": {
    "id": "cte28v1qpp-k"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference"
   ],
   "metadata": {
    "id": "8WXJEuyR-FHH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "atts_k = deb_atts.mean(axis=0).values\n",
    "att = np.mean(atts_k)\n",
    "\n",
    "# print(f\"Treatment effect of the treated using K={len(atts_k)}:\\n\")\n",
    "# print(\"atts_k:\", atts_k)\n",
    "# print(\"ATT:\", att)\n",
    "\n",
    "K = len(atts_k)\n",
    "T0 = len(y_pre_co)\n",
    "T1 = len(y_post_co)\n",
    "block_size = min(np.floor(T0/K), T1)\n",
    "se_hat=np.sqrt(1+((K*block_size)/T1))*np.std(atts_k, ddof=1)/np.sqrt(K)\n",
    "print(\"SE:\", se_hat)\n",
    "\n",
    "from scipy.stats import t\n",
    "alpha = 0.1\n",
    "# [att - t.ppf(1-alpha/2, K-1)*se_hat, att + t.ppf(1-alpha/2, K-1)*se_hat]"
   ],
   "metadata": {
    "id": "s4pbhSFL89ho"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Plots\n",
    "plt.plot(deb_atts.mean(axis=1).index, deb_atts.mean(axis=1).values, label=\"Debiased (Cross Validated) Effect\")\n",
    "# plt.plot(y_post_tr_variable.index, y_post_tr_variable-y0_hat_cov, color='black', linestyle='dashed', alpha=0.5, label=\"Original Effect\")\n",
    "\n",
    "#Indicative Lines\n",
    "plt.axvline(datetime.datetime(2024,4,10), color = 'black', linewidth=0.5)\n",
    "plt.axhline(0, color = 'black', linewidth=0.5)\n",
    "plt.axhline(att, color = 'tab:orange',linestyle='dashed', linewidth=1, label=\"Average Estimated Effect\")\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.title(\"Treatment effect\")\n",
    "plt.xlim([datetime.datetime(2024,4,10), datetime.datetime(2024,5,21)])\n",
    "plt.ylim([-0.1,0.1])\n",
    "plt.axhspan(att - t.ppf(1-alpha/2, K-1)*se_hat, att + t.ppf(1-alpha/2, K-1)*se_hat, alpha=0.05, color='red')\n",
    "plt.tick_params(axis='x', labelrotation=45)\n",
    "plt.savefig(\"Effect_withInference.svg\")\n",
    "# plt.show()"
   ],
   "metadata": {
    "id": "7Xh-yZ4mDSti"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Creating a dataframe for the weights for each holdout set\n",
    "weightsFrame = pd.DataFrame(columns=y_pre_co.columns)\n",
    "weightsFrame.index.names = [\"holdOutSet\"]\n",
    "for i in range(len(block_weights)):\n",
    "  weightsFrame.loc[i] = (block_weights[i])\n",
    "# display(weightsFrame)\n",
    "\n",
    "\n",
    "# Get unique column names and holdOutSet values\n",
    "columns = weightsFrame.columns\n",
    "holdout_sets = weightsFrame.index.unique()\n",
    "\n",
    "# Set up the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Set width of bars and positions of the bars on the x-axis\n",
    "bar_width = 0.1\n",
    "r = np.arange(len(columns))\n",
    "\n",
    "# Create a color map for holdOutSet\n",
    "color_map = plt.colormaps.get_cmap('Pastel1')\n",
    "colors = [color_map(i/len(holdout_sets)) for i in range(len(holdout_sets))]\n",
    "\n",
    "# Plot bars for each holdOutSet\n",
    "for i, holdout in enumerate(holdout_sets):\n",
    "    ax.bar(r + i * bar_width, weightsFrame.loc[holdout], width=bar_width,\n",
    "           label=holdout, color=colors[i])\n",
    "\n",
    "ax.set_ylabel('Weight')\n",
    "ax.set_title('Weights of the donor pool units for each cross validation set')\n",
    "ax.set_xticks(r + bar_width * (len(holdout_sets) - 1) / 2)\n",
    "ax.set_xticklabels(columns, rotation=360-35, ha='left')\n",
    "# ax.legend(title='Holdout Set')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"debiasedDist.svg\")\n",
    "# plt.show()"
   ],
   "metadata": {
    "id": "J4njV_nEfSFS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SankeyDiagram of the Dataflow"
   ],
   "metadata": {
    "id": "h-cuiyO_KX5r"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# sankeyData"
   ],
   "metadata": {
    "id": "7j4npREtV8ZX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# # Define the nodes\n",
    "# node_labels = [\"Complete Set\", \"Filter on Date\", \"Filter on Device_type\", \"Filter by manual exclusion\", \"Removed\", \"Used Data\"]\n",
    "# node_colors = [\"#1f77b4\", \"#ff7f0e\", \"#ff7f0e\", \"#ff7f0e\", \"#d62728\", \"#2ca02c\"]\n",
    "\n",
    "# # Define the links (source, target, value)\n",
    "# links = [\n",
    "#     #DateFilter\n",
    "#     (\"Complete Set\", \"Filter on Date\", sankeyData['complete']),\n",
    "#     (\"Filter on Date\", \"Removed\", sankeyData['complete']-sankeyData['datefilter']),\n",
    "#     #DeviceType Filter\n",
    "#     (\"Filter on Date\", \"Filter on Device_type\", sankeyData['datefilter']),\n",
    "#     (\"Filter on Device_type\", \"Removed\", sankeyData['datefilter']-sankeyData['deviceTypeFilter']),\n",
    "#     #Exclusion\n",
    "#     (\"Filter on Device_type\", \"Filter by manual exclusion\", sankeyData['deviceTypeFilter']),\n",
    "#     (\"Filter by manual exclusion\", \"Removed\", sankeyData['deviceTypeFilter']-sankeyData['manualExclusion']),\n",
    "#     #Finally Used\n",
    "#     (\"Filter by manual exclusion\", \"Used Data\", sankeyData['manualExclusion'])\n",
    "# ]\n",
    "\n",
    "# # Prepare data for Plotly\n",
    "# source_indices = [node_labels.index(link[0]) for link in links]\n",
    "# target_indices = [node_labels.index(link[1]) for link in links]\n",
    "# values = [link[2] for link in links]\n",
    "\n",
    "# # Create the Sankey diagram\n",
    "# fig = go.Figure(data=[go.Sankey(\n",
    "#     node = dict(\n",
    "#       pad = 15,\n",
    "#       thickness = 20,\n",
    "#       line = dict(color = \"black\", width = 0.5),\n",
    "#       label = node_labels,\n",
    "#       color = node_colors\n",
    "#     ),\n",
    "#     link = dict(\n",
    "#       arrowlen=0,\n",
    "#       source = source_indices,\n",
    "#       target = target_indices,\n",
    "#       value = values\n",
    "#   ))])\n",
    "\n",
    "# # Update the layout\n",
    "# fig.update_layout(title_text=\"Data flow\", font_size=11)\n",
    "\n",
    "# fig.update_layout(\n",
    "#     # Adjust overall font size here\n",
    "#     font=dict(size=14),\n",
    "#     # Increase top margin to accommodate labels above nodes\n",
    "#     margin=dict(t=50)\n",
    "# )\n",
    "\n",
    "# # Show the plot\n",
    "# fig.show()"
   ],
   "metadata": {
    "id": "lVTqoJlnWHit"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Altered Plotly plot to only report on percentages\n",
    "\n",
    "# Define the nodes\n",
    "node_labels = [\"Complete Set\", \"Filter on Date\", \"Filter on Device_type\", \"Filter by manual exclusion\", \"Removed\", \"Used Data\"]\n",
    "node_colors = [\"#1f77b4\", \"#ff7f0e\", \"#ff7f0e\", \"#ff7f0e\", \"#d62728\", \"#2ca02c\"]\n",
    "\n",
    "# Define the links (source, target, value)\n",
    "links = [\n",
    "    #DateFilter\n",
    "    (\"Complete Set\", \"Filter on Date\", sankeyData['complete']),\n",
    "    (\"Filter on Date\", \"Removed\", sankeyData['complete']-sankeyData['datefilter']),\n",
    "    #DeviceType Filter\n",
    "    (\"Filter on Date\", \"Filter on Device_type\", sankeyData['datefilter']),\n",
    "    (\"Filter on Device_type\", \"Removed\", sankeyData['datefilter']-sankeyData['deviceTypeFilter']),\n",
    "    #Exclusion\n",
    "    (\"Filter on Device_type\", \"Filter by manual exclusion\", sankeyData['deviceTypeFilter']),\n",
    "    (\"Filter by manual exclusion\", \"Removed\", sankeyData['deviceTypeFilter']-sankeyData['manualExclusion']),\n",
    "    #Finally Used\n",
    "    (\"Filter by manual exclusion\", \"Used Data\", sankeyData['manualExclusion'])\n",
    "]\n",
    "\n",
    "# Prepare data for Plotly\n",
    "source_indices = [node_labels.index(link[0]) for link in links]\n",
    "target_indices = [node_labels.index(link[1]) for link in links]\n",
    "values = [link[2] for link in links]\n",
    "\n",
    "# Calculate percentages of the complete set\n",
    "percentages = [value / sankeyData['complete'] * 100 for value in values]\n",
    "\n",
    "# Calculate percentages of the previous step\n",
    "previous_step_percentages = []\n",
    "for i, (source, target, value) in enumerate(links):\n",
    "    if i == 0:  # First link is always 100% of the previous step\n",
    "        previous_step_percentages.append(100)\n",
    "    else:\n",
    "        previous_value = next(link[2] for link in links if link[1] == source)\n",
    "        previous_step_percentages.append(value / previous_value * 100)\n",
    "\n",
    "# Create the Sankey diagram\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node = dict(\n",
    "      pad = 15,\n",
    "      thickness = 20,\n",
    "      line = dict(color = \"black\", width = 0.5),\n",
    "      label = node_labels,\n",
    "      color = node_colors,\n",
    "      hovertemplate='%{label}<extra></extra>'\n",
    "    ),\n",
    "    link = dict(\n",
    "      arrowlen=0,\n",
    "      source = source_indices,\n",
    "      target = target_indices,\n",
    "      value = values,\n",
    "      hovertemplate='%{source.label} → %{target.label}<br>' +\n",
    "                    'Percentage of complete set: %{customdata[0]:.2f}%<br>' +\n",
    "                    'Percentage of previous step: %{customdata[1]:.2f}%<extra></extra>',\n",
    "      customdata=list(zip(percentages, previous_step_percentages))\n",
    "  ))])\n",
    "\n",
    "# Update the layout\n",
    "fig.update_layout(title_text=\"Data flow\", font_size=11)\n",
    "fig.update_layout(\n",
    "    # Adjust overall font size here\n",
    "    font=dict(size=14),\n",
    "    # Increase top margin to accommodate labels above nodes\n",
    "    margin=dict(t=50)\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ],
   "metadata": {
    "id": "UecLMZKaLs6U"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# References\n",
    "> Facure, M. (2023). _Causal inference in Python: Applying Causal Inference in the Tech Industry_ (First Edition). O’Reilly.\n",
    "\n",
    "> Abadie, A. (2021). Using Synthetic Controls: Feasibility, Data Requirements, and Methodological Aspects. _Journal of Economic Literature_, _59_(2), 391–425.\n",
    "\n",
    ">\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "_nOPa2e5cyef"
   }
  }
 ]
}
